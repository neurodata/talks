<!DOCTYPE html>
<html>




<head>
  <title>PL</title>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <link rel="stylesheet" href="fonts/quadon/quadon.css">
  <link rel="stylesheet" href="fonts/gentona/gentona.css">
  <!-- <link rel="stylesheet" href="slides_style_v2.css"> -->
  <link rel="stylesheet" href="slides_style_i.css">
  <script type="text/javascript" src="assets/plotly/plotly-latest.min.js"></script>

</head>

<body>
  <textarea id="source">


#### Probably approximately correct in the future:<br> "Prospective Learning"

<br><br>
Joshua T. Vogelstein <br>
<!-- , [JHU](https://www.jhu.edu/) <br> -->
<!-- Co-PI: Vova Braverman, [JHU](https://www.jhu.edu/) <br> -->
Ashwin de Silva, Rahul Ramesh, Pratik Chaudhari
<!-- | Joshua T. Vogelstein <br> -->
<!-- [Microsoft Research](https://www.microsoft.com/en-us/research/): Weiwei Yang | Jonathan Larson | Bryan Tower | Chris White -->


<img src="images/neurodata_blue.png" width="20%" style="vertical-align: top; " >
<!-- <img src="images/jhu.png" width="8%" style="vertical-align: top"> -->

---

#### Outline 

- Motivation 
- Formalization
- Theorization
- Experimentation 
- Deliberation


---

# .center[Motivation]


---

#### Probably Almost Correct Learning 

--

- Nearly 100 years old

--

- Work horse of modern AI revolution

--

- Yet, the assumptions (IID) are wrong and dumb



---

#### What the &*#*@ is learning?

- Learning is an evolved property
- It enabled organisms to make better decisions about how to act *in the future* based on the past 
- This works because the future was (at least) partially predictable
- Biology evolved many different learning algorithms for different contexts


--

- What we call "learning" in AI is a formal model of a natural phenomenon
- And, there are many complimentary formal definitions, eg
    - PAC learning
    - Reinforcement learning
- But, it is not actually "learning" as observed in the world

---

#### Can we do any better?

- We start from a different assumption
  - Data model: random process, not  random variable 
  - Goal model:  dynamic objective, not  fixed objective
- This more complicated model reduces bias and adds variance
- Let's see some examples 
    - In each, $Z$ is a Bernoulli process


---

#### The classic

$Z_t$ is IID 

<img src="images/Zt_IID.png" width="640">

---

#### The  switcher

$Z_t$ is independent, but not identical  

<img src="images/Zt_s2.png" width="640">


---

#### The data dependent switcher

$Z_t$ neither independent nor identically distributed  

<img src="images/Zt_s3.png" width="640">


---

#### The decision dependent switcher

$Z_t$ depends on past data and decisions  

<img src="images/Zt_s4.png" width="640">


---

# .center[Formalization]


---

#### Data model 

- $z_t = (x_t, y_t) \in \mathcal{X} \times \mathcal{Y}$
- $z = (z\_t)\_{t \in \mathbb{N}}$ is a realization of a stochastic process $Z = (Z\_t)\_{t \in \mathbb{N}}$
- let $z\_{\leq t}$ denote the past and $z\_{>t}$ denote the future


---

#### Hypothesis class 

- a hypothesis sequence $h=(h\_t)\_{t \in \mathbb{N}}$
- $h\_t \in \mathcal{Y}^{\mathcal{X}}$ 
- $h \in \mathcal{H} \subset (\mathcal{Y}^{\mathcal{X}})^{\mathbb{N}}$ 


---

#### Learner 

- Map from data history to hypothesis sequence:
$$z_{\leq t} \mapsto h$$ 


---

#### Prospective loss, Risk, and expected Risk

- Prospective loss: 
$$
    \bar \ell(h, Z) =     \limsup\_{\tau \to \infty}    \frac{1}{\tau} \sum\_{s=1}^{\tau} \ell (s, h\_s(X\_s), Y\_s)
$$

where $\ell: \mathbb{N} \times  \mathcal{Y} \times \mathcal{Y} \mapsto [0,1]$ is a bounded, monotonically decaying (in time) loss function.

- Prospective risk at time $t$ is, for example, expected prospective loss

$$R\_t(h)
= \mathbb{E} [\bar \ell(h,Z) \mid z\_{\leq  t}] = \int \bar \ell(h,Z) \mathrm{d}{\mathbb{P}\_{Z \mid z\_{\leq t}}},$$

- Expected prospective risk at time $t$ integrates out the history

$$\mathbb{E} [R\_t(h)] = \int R\_t(h) \mathrm{d}{\mathbb{P}\_{Z\_{\leq t}}}$$


---

#### Prospective Bayes risk

A hypothesis sequence that achieves the minimal possible prospective risk, given the past, as a Bayes optimal hypothesis:

$$
    R\_t^* = \inf\_{h\in \sigma(Z\_{\leq t})}  R\_t(h)
$$

A Bayes optimal learner selects a  Bayes optimal hypothesis sequence at every time $t$.


---

#### Components  Prospective Learning  

- Data: $z = (z\_t)\_{t \in \mathbb{N}}$ is a realization of a stochastic process $Z = (Z\_t)\_{t \in \mathbb{N}}$ 

- Hypothesis sequence: $h=(h\_t)\_{t \in \mathbb{N}} \in \mathcal{H} \subset (\mathcal{Y}^{\mathcal{X}})^{\mathbb{N}}$, where  $h\_t \in \mathcal{Y}^{\mathcal{X}}$ 
  
- Learner: $z_{\leq t} \mapsto h$

- Prospective loss:
$
    \bar \ell(h, Z) =     \limsup\_{\tau \to \infty}    \frac{1}{\tau} \sum\_{s=1}^{\tau} \ell (s, h\_s(X\_s), Y\_s)
$


- Prospective risk: 
$R\_t(h)
= \mathbb{E} [\bar \ell(h,Z) \mid z\_{\leq  t}] = \int \bar \ell(h,Z) \mathrm{d}{\mathbb{P}\_{Z \mid z\_{\leq t}}},$

- Expected prospective risk:
$\mathbb{E} [R\_t(h)] = \int R\_t(h) \mathrm{d}{\mathbb{P}\_{Z\_{\leq t}}}$




---

#### Strong Prospective Learnability


<img src="images/strong_PL2.png" width="640">


Key differences with Strong PAC Learning:
- Risk is integrated over the future 
- Requires prospecting about (1) what the future will be like, and (2) what we will be like


---

#### Weak Prospective Learnability

<img src="images/weak_PL2.png" width="640">

<!-- where $L\_{ERM} : \mathcal{D} \mapsto \mathcal{H}$ be the ERM learner, so $\bar{h}\_0^{t'} =  L\_{ERM}(D\_{t'})$. -->

<!-- Key additional differences with Weak PAC Learning: -->
<!-- - we compare to an ERM learner, meaning it does not include time  -->


---

# .center[Theorization]


---

#### Time-Aware ERM 


---

#### Time-Aware ERM is a strong prospective learner 


---

#### There exists processes $Z$ for which time aware ERM strongly prospectively learns  


---

# .center[Experimentation]


---

#### Reversal Learning 


---

#### Algorithms 

- Time-agnostic MLP, CNN 
- Continual learner (fine-tuning)
- Time aware MLP, CNN, Transformer
- Oracle 

---

#### the switcher 

<img src="images/switcher.png" width="640">


---

#### the data dependent switcher 

<img src="images/ddswitcher.png" width="640">


---

# .center[Deliberation]


---

#### Isn't this just....


- time-series modeling / forecasting?
- online learning?
- continual/lifelong learning?
- online meta-learning?
- reinforcement learning?
- use a transformer for everything?

---

#### What's next? 

- Proving which kinds of stochastic processes are strongly/weakly prospectively learnable
- Developing algorithms that provably strongly/weakly prospectively learnable
- Implementing scalable algorithms
- Deploying algorithms in real-world applications 


---

#### Publications


1. De Silva et al. [The Value of Out-of-Distribution Data](https://arxiv.org/abs/2109.14501), ICML, 2023.
1. De Silva et al. [Prospective Learning: Principled Extrapolation to the Future](https://arxiv.org/abs/2004.12908), CoLLAs, 2023.
1. De Silva et al. Prospective Learning: Learning for a Dynamic Future, [preprint available upon request](mailto:joshuav@gmail.com).



---
##### Acknowledgements


<img src="images/neurodata2023.jpg" width="640">


.small[NSF Simons MoDL, ONR N00014-22-1-2255, and NSF CCF 2212519]

---


##### Questions?



<img src="images/dino_yummies.jpg" width="640">




</textarea>
  <!-- <script src="https://gnab.github.io/remark/downloads/remark-latest.min.js"></script> -->
  <!-- <script src="remark-latest.min.js"></script> -->
  <script src="remark-latest.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/contrib/auto-render.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css">
  <script type="text/javascript">

    var options = {};
    var renderMath = function () {
      renderMathInElement(document.body);
      // or if you want to use $...$ for math,
      renderMathInElement(document.body, {
        delimiters: [ // mind the order of delimiters(!?)
          { left: "$$", right: "$$", display: true },
          { left: "$", right: "$", display: false },
          { left: "\\[", right: "\\]", display: true },
          { left: "\\(", right: "\\)", display: false },
        ]
      });
    }

    // remark.macros.scale = function (percentage) {
    //   var url = this;
    //   return '<img src="' + url + '" style="width: ' + percentage + '" />';
    // };

    // var slideshow = remark.create({
    // Set the slideshow display ratio
    // Default: '4:3'
    // Alternatives: '16:9', ...
    // {
    // ratio: '16:9',
    // });

    var slideshow = remark.create(options, renderMath);


  </script>
</body>

</html>
