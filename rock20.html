<!DOCTYPE html>
<html>
  <head>
    <title>rock20</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <link rel="stylesheet" href="fonts/quadon/quadon.css">
    <link rel="stylesheet" href="fonts/gentona/gentona.css">
    <link rel="stylesheet" href="slides_style.css">
    <script type="text/javascript" src="assets/plotly/plotly-latest.min.js"></script>
  </head>
  <body>
    <textarea id="source">



class: inverse

---
class: inverse

## What is Learning?


--

<br>
"The acquisition of knowledge or skills through experience, study, or by being taught."

-- Google, 2020


---
class: inverse

## Consider a simple example

- 2 classes: green and purple
- we observe 100 points  
- each point has 2 features: dim1 and dim2
- we desire to .r[learn] a classifier that discriminates between these two classes


---
class: inverse

<img src="images/rock20/s2.png" style="position:absolute; top:0px; left:100px; height:100%;"/>




---
class: inverse

<img src="images/rock20/s3.png" style="position:absolute; top:0px; left:100px; height:100%;"/>



---
class:inverse

## What do classifiers do?


---

<img src="images/rock20/sphx_glr_plot_lda_qda_001.png" class="center"/>


---

<img src="images/rock20/sphx_glr_plot_iris_svc_001.png" class="center"/>


---

<img src="images/rock20/sphx_glr_plot_nearest_centroid_001.png" class="center"/>

---

<img src="images/rock20/sphx_glr_plot_classification_001.png" class="center"/>

---

<img src="images/rock20/sphx_glr_plot_iris_dtc_001.png" class="center"/>

---

<img src="images/rock20/sphx_glr_plot_forest_iris_001.png" class="center"/>



---
class:inverse

## What do classifiers do?

--
<br>

train:
1. partition feature space into "parts",
2. the majority vote of points in the part determines its class.

predict: 
2. for a new unlabeled point, find the its part,
3. report the majority vote in its part.






---
class:inverse


---
class:inverse 

## Do brains do it?

--

(brains obviously learn)

1. Do brains partition feature space?
2. Is there some kind of "voting" occurring within each part?


---
class: inverse

## Brains partition space 


1. visual receptive fields
2. place fields / grid cells
3. sensory homonculus

<br>

<img src="images/rock20/Side-black.gif" style="height:230px;"/>
<img src="images/rock20/Front_of_Sensory_Homunculus.gif" style="height:230px;"/>
<img src="images/rock20/Rear_of_Sensory_Homunculus.jpg" style="height:230px;"/>


---
class: inverse

## Neurons vote

<img src="images/rock20/brody1.jpg" style="height:500px;"/>




---
class: inverse 


## Summary so far 

<br>

1. learning is partitioning space and then voting 
2. brains do both things


--

<br>
satisfied?


---
class: inverse

## What is Learning?

--

<br>
"Learning is the process of acquiring new, .r[or modifying existing], knowledge, behaviors, skills, values, or preferences."

-- Wikipedia



---
class: inverse

<img src="images/rock20/s3.png" style="position:absolute; top:0px; left:100px; height:100%;"/>


---
class: inverse

<img src="images/rock20/s3.png" style="position:absolute; top:0px; left:100px; height:100%; transform:rotate(30deg);"/>


---
class: inverse, middle


Shouldn't learning be .r[faster] if we already learned a related task?

--

Yes. This is called "transfer learning".


---
class: inverse 

## Honey Bees can do it


<img src="images/rock20/honeybee1.jpg" style="position:absolute; top:150px; width:80%; "/>

--
<br><br><br><br><br><br><br><br><br><br><br>
- honey bees can also transfer to different sensory modality (smell)
- honeybees do not forget how to do the first task
- this is called "forward transfer"

---
class: inverse 

## Can AI do it?

--

"Training on a new set of items may drastically disrupt performance on previously learned items."

-- McCloskey & Cohen, 1989

---
class: inverse 

## 30 years later... 

<img src="images/rock20/masse1.png" style="width:600px;"/>
<img src="images/rock20/flesch1.png" style="width:600px;"/>
<img src="images/rock20/kirkpatrick1.png" style="width:600px;"/>


---
class: inverse 

## A Grand Challenge

"Achieving artificial general intelligence requires that agents
are able to learn and remember many different tasks." -- Kirkpatrick et al. (PNAS, 2017)


"To get there, we will need something else too: a fundamental rethinking of how learning works. We need to invent a new kind of learning that leverages existing knowledge, rather than one that obstinately starts over from square one in every domain it confronts.”— Rebooting AI: Building Artificial Intelligence We Can Trust by Gary Marcus, Ernest Davis



---
class:inverse 

## Consider an  example

- *CIFAR 100* is a popular image classification dataset with 100 classes of images, containing 600 images, each 32x32 colour images.
- There are 500 training images and 100 testing images per class.
- CIFAR 10x10 breaks the 100 class task problem into 10 problems, each a 10 class problem.

<img src="images/l2m_18mo/cifar-10.png" style="position:absolute; left:250px; width:400px;"/>


---
class: inverse 


## Current State-of-the-Art


<img src="images/l2m_18mo/progressive_nets.png" style="width:650px;"/>

Seungwon Lee, James Stokes, and Eric Eaton. "[Learning Shared Knowledge for Deep Lifelong Learning Using Deconvolutional Networks](https://www.ijcai.org/proceedings/2019/393)." IJCAI, 2019.


---
class: inverse 

## Problem solved?

--

not quite. 


---
class: inverse 

## What about .r[reverse transfer]?


- In general, tasks are complex, built of sub-tasks, if learning a new complex task can improve sub-task performance (including sub-tasks involved in previous tasks), then performance in previous tasks will improve too.

- "Knowledge and skills from a learner’s first language are used and reinforced, deepened, and expanded upon when a learner is engaged in second language literacy tasks." -- [American Council on the Teaching of Foreign Languages](https://www.actfl.org/guiding-principles/literacy-language-learning)



---
class:inverse 

## Summary of transfer learning

- Modern AI can forward transfer
- Modern AI **cannot** reverse transfer 
- Biological intelligence can do both
- AI should do both

---
class:inverse 


---
class:inverse 

## A very simple agent

<img src="images/rock20/L2F-RF.svg" style="width:650px;">

--


- Problem: gerrymandering


---
class:inverse 

## An honest agent

<img src="images/rock20/L2F-UF.svg" style="width:650px;">

--

- in ML, a "calibrated" learner is one who consistently provides the probability of a given sample is in each class 
- most ML algorithms are *not* calibrated 
- this includes all Deep Learning algorithms ever, to my knowledge



---
class:inverse 

## A  transfer agent

<img src="images/rock20/L2F-TF.svg" style="width:650px;">

--

- each task gets a unique partition
- task 2 uses both partitions



---
class:inverse 

## A  lifelong learning agent

<img src="images/rock20/L2F-LF.svg" style="width:650px;">

--

- each task gets a unique partition
- all tasks use all partitions 
- any task can benefit from any other task if partition is relevant



---
class: inverse

## An illustrative example: XOR 

- samples in the (+, +) and (-, -) quadrants are class 1  
- samples in the (-, +) and (+, -) quadrants are class 0 
- the optimal decision boundary is the coordinate axes 
- we want to classify black dot

<img src="images/rock20/xor.png" style="width:400px"/>



---
class: inverse

##  Honest Forests
  
<img src="images/l2m_18mo/just_xor.png"  style="height:300px;">

- error is estimated generalization error
- partition learned using a subset of data from XOR 


---
class: inverse

##  Honest Forests

<img src="images/l2m_18mo/just_xor.png" alt="Snow" style="height:300px;">
<img src="images/rock20/posterior_heatmap_xor_nxor_rf.png" style="height:300px;">

- error is estimated generalization error
- partition learned using a subset of data from XOR 
- votes happened using other XOR data



---
class: inverse

## A Transfer Example

- .r[XOR]
  - Samples in the (+, +) and (-, -) quadrants are green  
  - samples in the (-, +) and (+, -) quadrants are purple 
- .lb[N-XOR]
  - Samples in the (+, +) and (-, -) quadrants are green  
  - samples in the (-, +) and (+, -) quadrants are purple. 
- Optimal decision boundary for both problems are  coordinate axes

<img src="images/l2m_18mo/xor_nxor.png" style="width:475px" class="center"/> 

---
class: inverse
## Transfer  Forests

<div style="display:inline-flex">
     <div>
          <img src="images/l2m_18mo/L2M_18mo_xor_nxor_transfer.png" alt="Snow" style="height:300px; width:300px">
     </div>
     <div>
          <img src="images/l2m_18mo/posterior_heatmap_xor_nxor_transfer.png" alt="nah" style="height:300px; width:300px">
     </div>
</div>

- (Left)  Decision Forest .r[XOR] uses:
  - 100 samples from .r[XOR] to vote
  - $n$ data from .lb[N-XOR] to learn partition
- (Right) learned probability of an observation  being green
  - used 100 .r[XOR]  and 300 .lb[N-XOR] training samples 

---
class:inverse 
## Transfer Forests

<div style="display:inline-flex">
     <div>
          <img src="images/l2m_18mo/L2M_18mo_xor_nxor_transfer.png" alt="Snow" style="height:300px; width:300px">
     </div>
     <div>
          <img src="images/l2m_18mo/posterior_heatmap_xor_nxor_transfer.png" alt="nah" style="height:300px; width:300px">
     </div>
</div>

- Learning partitions is "harder" than posteriors:
  - DF .r[XOR] only gets 100 samples to learn partitions,  
  - DF .lb[N-XOR] gets up to 400
- DF .lb[N-XOR] exhibits transfer learning, because its error decreases on .r[XOR] as the number of samples from .lb[N-XOR] increases for a fixed amount of data from .r[XOR] 




---
class: inverse

## Lifelong Forests 

<div style="display:flex">
      <div>
          <img src="images/l2m_18mo/L2M_18mo_xor_nxor_lifelong.png" alt="Snow" style="height:300px; width:300px">
     </div>
     <div>
          <img src="images/l2m_18mo/posterior_heatmap_xor_nxor_lifelong.png" alt="nah" style="height:300px; width:300px">
     </div>
</div>

- .green[Lifelong Forests] uses partitions from  both 
  -  100 samples from .r[XOR], and 
  -  $n$ samples from  .lb[N-XOR].
- Posteriors are  learned using only 300 samples from .lb[N-XOR] on both partitions.
- Prediction is based on  the  learned posterior averaged over both partitions.




---
class:inverse

## "Real" data example

- same CIFAR experiment as before
  -  10  tasks, each with 10 classes, 500 training samples / class
- y-axis indicates .r[transfer efficiency] (TE), which is the ratio of "single task error" to "error using other tasks"
- we want the numbers to be >1
- each task will have a line
  - if the line starts >1, that means it is doing "forward transfer"
  - if the line .r[increases], that means it is doing "reverse transfer"


---
class:inverse 

<img src="images/rock20/EWC_CIFAR.png" style="height:530px;"/>

DeepMind's "Elastic Weight Consolidation"


---
class:inverse 

<img src="images/rock20/SI_CIFAR.png" style="height:530px;"/>

Surya Ganguli's "Synaptic Intelligence"

---
class:inverse

<img src="images/l2m_18mo/cifar-100-TLE-Accuracy.png" style="height:530px;"/>



- TE starting >1 indicates forward transfer
- TE increasing with more tasks indicates reverse trasfer



---
class: inverse 


## Summary of Experiments 

<br> 

Lifelong forest is the only AI that exhibits reverse transfer. 

---
class: inverse



---
class: inverse 

## Models of the Brain 

- there are many 
- we are proposing one for how brains learn efficiently, especially with respect to sequential task learning
- the main utility of a model is in suggesting the next experiment to perform 


---
class: inverse

## Next experiment to perform

Experiment:
- select a species that can transfer learn 
- teach an organism task 0 (source task)
- teach it one of two "target tasks"
  - target task A is very similar to source task
  - target task B is very different from source task

Predictions:
1. same neurons will be activated for task 0 and task 1
1. those neurons will exhibit distinct firing patterns for the two tasks
2. different neurons will be activated for tasks 0 and task 2


---
class: inverse, middle

There are many models of the brain, in which ways is this one better?


---
class: inverse

## What is Learning (formally)?


<img src="images/Vapnik71b.png" style="width:400px;"/>
<img src="images/Valiant84.png" style="width:400px;"/>
<img src="images/Mitchell97a.png" style="width:400px;"/>


---
class: inverse

## What is  Learning (formally)? 


"An agent $f$  learns from data $D_n$ with respect to task $T$ with performance measure $\mathcal{E}$, if $f$'s performance at task  $T$ improves with $D_n$.''

-- Tom Mitchell, 1997 (not exact quote)


---
class:inverse 


## What is Learning (formally)?


Generalization error $\mathcal{E}$ is the expected risk with respect to  training dataset:

$$ \mathcal{E}\_T(f\_n) = \mathbb{E}_{P}[R(f_n(D_n))].$$


<br>

$f$ learns from data iff $\mathcal{E}_T(f_n)$ is better than $\mathcal{E}_T(f_0)$, or more formally

<br>

$$\frac{\mathcal{E}_T(f_0)}{\mathcal{E}_T(f_n)} > 1.$$ 

--

-- jovo, 2019


---
class:inverse

## The fundamental theorem of statistical pattern recognition

--

If each cell is:

1. small enough, and 
2. has enough points in it, 

then given enough data, one can learn *perfectly, no matter what*! 


-- Stone, 1977




---
class: inverse

## What is Transfer Learning? 

- Given
  - data $D_0$, and an algorithm $f_0$ trained using only $D_0$. 
  - data $D_n$ from possibly another distribution, 
  - algorithm $f_n$ trained using both $D_0$ and $D_n$



"An algorithm $f$  .r[transfer] learns from data $D_n$  with respect to transfer learning task $T$ with performance measure $\mathcal{E}$, if $f$'s performance at task  $T$ improves over  with $D_n$.'"

--


$f$ .r[transfer] learns from data iff 

$$\frac{\mathcal{E}\_T(f\_0)}{\mathcal{E}\_T(f\_{n})} > 1.$$


---
class:inverse

## The fundamental .r[conjecture] of transfer learning

--

If each cell is:

- small enough, and 
- has enough points in it, 

then given enough data, one can .r[transfer learn] *perfectly, no matter what*! 


-- jovo, 2020

--

In other words, 
- if brains brains transfer learn as described (partitions and voting), 
- then they can transfer learn in essentially any scenario.


---
class: inverse

##  What next?

- Nothing really special about "forests", could apply the same idea to any deep net.
- Bees transfer across modalities, we don't know how to do that.


---
class: inverse

## Total Summary  

- Learning can be done by partitioning + voting
- Brains learn this way
- Brains can transfer learn
- Existing DeepNets can only forward transfer 
- Lifelong Forests can also reverse transfer 
- Neuro experiments suggested based on model 
- Theory promising to prove partitioning + voting is basically required


---
class:inverse 

# Lifelong Learning

Joshua T. Vogelstein, BME@JHU



---
class: inverse

## References 

3. T. M. Tomita et al. [Sparse  Projection Oblique Randomer Forests](https://arxiv.org/abs/1506.03410). arXiv, 2018.
1. R Guo, et al. [Estimating Information-Theoretic Quantities with Uncertainty Forests](https://arxiv.org/abs/1907.00325). arXiv, 2019.
1. R. Perry, et al. Manifold Forests: Closing the Gap on Neural Networks. preprint, 2019.
1. C. Shen and J. T. Vogelstein. [Decision Forests Induce Characteristic Kernels](https://arxiv.org/abs/1812.00029). arXiv, 2018
7. J. Browne et al. [Forest Packing: Fast, Parallel Decision Forests](https://arxiv.org/abs/1806.07300). SIAM ICDM, 2018.
1. M. Madhya, et al. [Geodesic Learning via Unsupervised Decision Forests](https://arxiv.org/abs/1907.02844). arXiv, 2019.
1. H. Helm et al. Lifelong Learning Forests, 2020
1. R. Mehta et al. A General Theory of Learnability, 2020. 

Code: [https://neurodata.io/sporf/](https://neurodata.io/sporf/)




---
class:inverse 

## Acknowledgements



<!-- <div class="small-container">
  <img src="faces/ebridge.jpg"/>
  <div class="centered">Eric Bridgeford</div>
</div>

<div class="small-container">
  <img src="faces/pedigo.jpg"/>
  <div class="centered">Ben Pedigo</div>
</div>

<div class="small-container">
  <img src="faces/jaewon.jpg"/>
  <div class="centered">Jaewon Chung</div>
</div> -->


<div class="small-container">
  <img src="faces/yummy.jpg"/>
  <div class="centered">yummy</div>
</div>

<div class="small-container">
  <img src="faces/lion.jpg"/>
  <div class="centered">lion</div>
</div>

<div class="small-container">
  <img src="faces/violet.jpg"/>
  <div class="centered">baby girl</div>
</div>

<div class="small-container">
  <img src="faces/family.jpg"/>
  <div class="centered">family</div>
</div>



<div class="small-container">
  <img src="faces/earth.jpg"/>
  <div class="centered">earth</div>
</div>


<div class="small-container">
  <img src="faces/milkyway.jpg"/>
  <div class="centered">milkyway</div>
</div>

<br>




<div class="small-container">
  <img src="faces/cep.png"/>
  <div class="centered">Carey Priebe</div>
</div>

<div class="small-container">
  <img src="faces/randal.jpg"/>
  <div class="centered">Randal Burns</div>
</div>


<div class="small-container">
  <img src="faces/cshen.jpg"/>
  <div class="centered">Cencheng Shen</div>
</div>


<!-- <div class="small-container">
  <img src="faces/bruce_rosen.jpg"/>
  <div class="centered">Bruce Rosen</div>
</div>


<div class="small-container">
  <img src="faces/kent.jpg"/>
  <div class="centered">Kent Kiehl</div>
</div> -->

<!-- <div class="small-container">
  <img src="faces/mim.jpg"/>
  <div class="centered">Michael Miller</div>
</div>

<div class="small-container">
  <img src="faces/dtward.jpg"/>
  <div class="centered">Daniel Tward</div>
</div> -->


<!-- <div class="small-container">
  <img src="faces/vikram.jpg"/>
  <div class="centered">Vikram Chandrashekhar</div>
</div>


<div class="small-container">
  <img src="faces/drishti.jpg"/>
  <div class="centered">Drishti Mannan</div>
</div> -->

<div class="small-container">
  <img src="faces/jesse.jpg"/>
  <div class="centered">Jesse Patsolic</div>
</div>

<div class="small-container">
  <img src="faces/falk_ben.jpg"/>
  <div class="centered">Benjamin Falk</div>
</div>

<!-- <div class="small-container">
  <img src="faces/kwame.jpg"/>
  <div class="centered">Kwame Kutten</div>
</div> -->

<!-- <div class="small-container">
  <img src="faces/perlman.jpg"/>
  <div class="centered">Eric Perlman</div>
</div> -->

<!-- <div class="small-container">
  <img src="faces/loftus.jpg"/>
  <div class="centered">Alex Loftus</div>
</div> -->

<!-- <div class="small-container">
  <img src="faces/bcaffo.jpg"/>
  <div class="centered">Brian Caffo</div>
</div> -->

<!-- <div class="small-container">
  <img src="faces/minh.jpg"/>
  <div class="centered">Minh Tang</div>
</div> -->

<!-- <div class="small-container">
  <img src="faces/avanti.jpg"/>
  <div class="centered">Avanti Athreya</div>
</div> -->

<!-- <div class="small-container">
  <img src="faces/vince.jpg"/>
  <div class="centered">Vince Lyzinski</div>
</div> -->

<!-- <div class="small-container">
  <img src="faces/dpmcsuss.jpg"/>
  <div class="centered">Daniel Sussman</div>
</div> -->

<!-- <div class="small-container">
  <img src="faces/youngser.jpg"/>
  <div class="centered">Youngser Park</div>
</div> -->

<!-- <div class="small-container">
  <img src="faces/shangsi.jpg"/>
  <div class="centered">Shangsi Wang</div>
</div> -->

<div class="small-container">
  <img src="faces/tyler.jpg"/>
  <div class="centered">Tyler Tomita</div>
</div>

<div class="small-container">
  <img src="faces/james.jpg"/>
  <div class="centered">James Brown</div>
</div>

<!-- <div class="small-container">
  <img src="faces/disa.jpg"/>
  <div class="centered">Disa Mhembere</div>
</div> -->

<!-- <div class="small-container">
  <img src="faces/gkiar.jpg"/>
  <div class="centered">Greg Kiar</div>
</div> -->


<!-- <div class="small-container">
  <img src="faces/jeremias.png"/>
  <div class="centered">Jeremias Sulam</div>
</div> -->


<div class="small-container">
  <img src="faces/meghana.png"/>
  <div class="centered">Meghana Madhya</div>
</div>
  

<!-- <div class="small-container">
  <img src="faces/percy.png"/>
  <div class="centered">Percy Li</div>
</div>
-->

<div class="small-container">
  <img src="faces/hayden.png"/>
  <div class="centered">Hayden Helm</div>
</div>


<div class="small-container">
  <img src="faces/rguo.jpg"/>
  <div class="centered">Richard Gou</div>
</div>

<div class="small-container">
  <img src="faces/ronak.jpg"/>
  <div class="centered">Ronak Mehta</div>
</div>

<div class="small-container">
  <img src="faces/jayanta.jpg"/>
  <div class="centered">Jayanta Dey</div>
</div>



</div>
<!-- <img src="images/funding/nsf_fpo.png" STYLE="HEIGHT:95px;"/> -->
<!-- <img src="images/funding/nih_fpo.png" STYLE="HEIGHT:95px;"/> -->
<img src="images/funding/darpa_fpo.png" STYLE=" HEIGHT:95px;"/>
<!-- <img src="images/funding/iarpa_fpo.jpg" STYLE="HEIGHT:95px;"/> -->
<!-- <img src="images/funding/KAVLI.jpg" STYLE="HEIGHT:95px;"/> -->
<!-- <img src="images/funding/schmidt.jpg" STYLE="HEIGHT:95px;"/> -->

---
class:center, inverse

<img src="images/l_and_v.jpeg" style="position:absolute; top:0px; left:200px; height:100%;"/>

---
class: middle, inverse


## .center[Extra Slides]




---
class: inverse

## Adversarial Tasks 

- .r[XOR] vs .lb[N-XOR] is the easiest possible sequence of tasks 
- Consider an adversarial task: 
  - a second task, .lb[Rotated-XOR (R-XOR)], which has zero information about the first task; 
  - therefore, its data can only add noise and increase error 

<img src="images/rock20/xor_rxor.png" style="width:475px" class="center"/>

<!-- add two linear two-class classification problem w adverserial source task -->

<!-- adverserial source task = either means reflected across x-axis or two gaussians centered at 0,0 -->

---
class:inverse 

## An Adversarial Example


<!-- integrate this content into slide with figure -->
- Left figures: 
  - y-axis is  error on .r[XOR] learned using 100 samples 
  - x-axis is # of .lb[R-XOR] data samples
  - we used 100 .r[XOR] data samples
- Right figures:
  - estimated probability of an observation from purple
  - learned using 100 .r[XOR]  and 300 .lb[R-XOR] training samples


---
class:inverse  

## Decision Forests 

<div style="display:flex">
     <div>
          <img src="images/l2m_18mo/L2M_18mo_xor_rxor_rf.png" alt="Snow" style="height:300px; width:300px">
     </div>
     <div>
          <img src="images/l2m_18mo/posterior_heatmap_xor_rxor_rf.png" alt="nah" style="height:300px; width:300px">
     </div>
</div>

- Decision Forest .r[XOR] uses 
  - data from .r[XOR] to learn  partition
  - other data from .r[XOR] to vote
- Since the number of samples from .r[XOR] is fixed, the error of DF .r[XOR] is constant


---
class: inverse 

## Transfer Forests 

<div style="display:flex">
     <div>
          <img src="images/l2m_18mo/L2M_18mo_xor_rxor_transfer.png" alt="Snow" style="height:300px; width:300px">
     </div>
     <div>
          <img src="images/l2m_18mo/posterior_heatmap_xor_rxor_transfer.png" alt="nah" style="height:300px; width:300px">
     </div>
</div>

- Decision Forest  .lb[R-XOR] uses 
  - data from  .lb[R-XOR] to learn partition
  - data from .r[XOR] to vote
- Error only decreases slightly because the partitions are nearly uninformative (the informativeness is actually due to noisy estimate of the partition, the optimal partition is fully uninformative)

<!-- fix posterior plots as above  -->

---
class:inverse
## Lifelong Forests

<div style="display:flex">
     <div>
          <img src="images/l2m_18mo/L2M_18mo_xor_rxor_lifelong.png" alt="Snow" style="height:300px; width:300px">
     </div>
     <div>
          <img src="images/l2m_18mo/posterior_heatmap_xor_rxor_lifelong.png" alt="nah" style="height:300px; width:300px">
     </div>
</div>

- .green[Lifelong Forests] learn the optimal decision boundary using  data from both (1) .r[XOR], and (2)  .lb[R-XOR]. 
- Posteriors are  learned using only data from .r[XOR] on both partitions. 
- Prediction is based on  the  estimated posterior averaged over both partitions.


---
class: inverse

## Polytope space partitioning algorithms


<img src="images/deep-polytopes.png" style="width:750px;"/>


- NNs with ReLu nodes also partitions feature space into  polytopes ([NIPS, 2014](http://papers.nips.cc/paper/5422-on-the-number-of-linear-regions-of-deep-neural-networks.pdf)).



---

### RF is more computationally efficient 


<img src="images/s-rerf_6plot_times.png" style="width:750px;"/>


---



### Lifelong learning algorithms
- Fundamentally, a lifelong learning algorithm must summarize previous tasks via a useful representation to effectively use these tasks for learning a new task. 

- In supervised classification, the most useful representation of a task is its corresponding optimal decision boundary.

- More generally, tessellating space into cells yields a representation/compression valuable for any subsequence inference task.

---

### What are Decision Forests?

- Two things:
  1. a partitioning of the space into mutually exclusive cells 
  2. an assignment of probabilities  within each cell


- This is true for decision trees, random forests, gradient boosting trees, etc.	
  - Axis-aligned trees yield cells that are each right rectangular prisms (also called rectangular cuboids, rectangular parallelepiped, and orthogonal parallelipiped)	
  - Axis-oblique trees yield cells that are more general polytopes

Note: Deep Nets with ReLu activation function also tesselate space into polytopes  



---

### Key Algorithmic Insight 

- Cells of a partition of space can be a universal representation
- Partitions can be learned on a **per task** basis
- Probabilities can be estimated given **any** partition, including those from other tasks
- Use all data to estimate probabilities on the partitions learned separately for each task
- Average probabilities learned on each partition to obtain final answer


---
### Algorithm (High-Level)

- Input: $n$ points associated with $J$ different supervised classification/regression tasks
- Output: A classifier/regressor function for each task 
- Algorithm: 
      - for $j \in [J]$ 
        - Learn partition of space using only data from task $j$
        - Learn probabilities using partitions $1, \ldots, j$ 
        - Output average over all $j$ probability estimates 

--

  Notes:

- This exact procedure can be applied as $J$ continues to increase
- This procedure could be use any pair of algorithms for learning partitions and probabilities. 
- We focus on random forests for simplicity.
- We assumed related measurement spaces, specifically, $\mathcal{Z}\_j= \lbrace \mathcal{X} \times \mathcal{Y}\_j \rbrace$



---

### Basic L2M (unbounded storage)

<!-- Given the above assumptions, all lifelong learning algorithms can be characterized as follows. -->


For each $(z_t, j_t)$,
1. update .pu[transformer]  using all available data: $h\_t (\lbrace z\_t,j\_t \rbrace) \rightarrow \hat{f}\_t$ 
2. for each sample, apply .pu[transformer]: $\hat{f}\_t (z\_i) = \tilde{z}\_i$
3. for each  .pu[decider], update using all available data: 
      $g\_t (\lbrace \tilde{z}\_t,j\_t \rbrace) \rightarrow  \lbrace \hat{\eta}\_j \rbrace $
4. apply .pu[decider]: $\hat{\eta}\_{j\_t}(\tilde{z}\_t) \rightarrow a\_t$



---

### Basic L2M (bounded storage)



For each $(z_t, j_t)$,
1. update .pu[transformer]: $h\_t ( z\_t,j\_t, \hat{f}\_{t-1} ) \rightarrow \hat{f}\_t$ 
2. apply .pu[transformer]: $\hat{f}\_t (z\_t) = \tilde{z}\_t$
3. update all .pu[deciders]: 
      $g\_t ( \tilde{z}\_t,j\_t, \hat{\eta}\_{t-1} ) \rightarrow  \lbrace \hat{\eta}\_j \rbrace $
4. apply .pu[decider]: $\hat{\eta}\_{j\_t}(\tilde{z}\_t) \rightarrow a\_t$



---

### Lifelong  Forests 

- Assume data are batched into unique tasks
- For each task, $ z_t | j_t = j$
  1. Learn a new forest using data only from task $j_t$
  2. Apply forest to each observation so far 
  3. For each task, update plurality/average vote per tree 
  4. Apply average vote for $z_t$


---

# Universal Slides 



---

### What is Universal Transfer Learning?

We say that $f$ **universally** transfers in setting $S$ iff 
$\exists \; \delta, n\_0 > 0$ such that $\forall n\_j > n\_0$:

<!-- $\forall n,n' > n\_0$, $\exists \; \delta > 0$ such that: -->



$\exists P,Q \in \mathcal{P}: TLE\_{n,n'}^{P,Q}(f) <  1 - \delta$ (sometimes better), and

$\forall P,Q \in \mathcal{P}: TLE\_{n,n'}^{P,Q}(f) \leq  1 + \delta$  (never much worse).


---

### What is Universal MT Learning?

We say that $f$ **universally** multi-task learns in settings $\vec{S}$ iff
$\exists \; \delta, n\_0 > 0$ such that $\forall n\_j > n\_0$:


$\exists \vec{P} \in \vec{\mathcal{P}}: MTE\_{\vec{n}}^{\vec{P}}(f) <  1 - \delta$  (sometimes better), and

$\forall \vec{P} \in \vec{\mathcal{P}}: MTE\_{\vec{n}}^{\vec{P}}(f) \leq  1 + \delta$ (never much worse).




---

### What is Universal Lifelong Learning?


We say that $f$ **universally** lifelong learns in settings $\vec{S}$ iff $\forall n\_j > n\_0$ and $\forall \vec{P} \in \vec{\mathcal{P}}$, lifelong learning holds.



---

## Consider the iris dataset

- introduced by RA Fisher (inventor of modern statistics) 
- 150 points from 3 species of iris's
  - setosa, virginica, versicolor
- 4 dimensions/features: 
  - sepal length, sepal width, petal length, petal width 



---
class:inverse

<img src="images/rock20/sphx_glr_plot_iris_dtc_001.png" style="position:absolute; top:0px; left:0px; height:100%;"/>



---
class: inverse

<img src="images/rock20/s2.png" style="position:absolute; top:0px; left:100px; height:100%;"/>/>


---
class: inverse

<img src="images/rock20/s3.png" style="position:absolute; top:0px; left:100px; height:100%;"/>

---
class: inverse

---
class: inverse

<img src="images/rock20/s2.png" style="width:600px;", style="position:absolute; top:0px; left:100px; height:100%;"/>/>


---
class: inverse

<img src="images/rock20/s3.png" style="width:600px;", style="position:absolute; top:0px; left:100px; height:100%;"/>/>

---
class: inverse

<img src="images/rock20/s4.png" style="width:600px;", style="position:absolute; top:0px; left:100px; height:100%;"/>/>


---
class: inverse

<img src="images/rock20/s5.png" style="width:600px;", style="position:absolute; top:0px; left:100px; height:100%;"/>/>


---
class: inverse

<img src="images/rock20/s6.png" style="width:600px;", style="position:absolute; top:0px; left:100px; height:100%;"/>/>

---
class: inverse

<img src="images/rock20/s7.png" style="width:600px;", style="position:absolute; top:0px; left:100px; height:100%;"/>/>

---
class: inverse

<img src="images/rock20/s8.png" style="width:600px;", style="position:absolute; top:0px; left:100px; height:100%;"/>/>


---
class: inverse

<img src="images/rock20/s9.png" style="width:600px;", style="position:absolute; top:0px; left:100px; height:100%;"/>/>



---

### What is Transfer Learning? 

- Given
  - data $D_0$, and an algorithm $f_0$ trained using only $D_0$. 
  - data $D_n$ from possibly another distribution, 
  - algorithm $f_n$ trained using both $D_0$ and $D_n$



"An algorithm $f$  .pu[transfer] learns from data $D_n$  with respect to transfer learning task $T$ with performance measure $\mathcal{E}$, if $f$'s performance at task  $T$ improves over  with $D_n$.'"

--


$f$ .pu[transfer] learns from data iff 

$$\frac{\mathcal{E}\_T(f\_0)}{\mathcal{E}\_T(f\_{n})} > 1.$$



---

### What is Multi-Task (MT) Learning?

- Given
  - for $j=1,\ldots, J$ 
      - data $D_j$, possibly from different distributions, 
      - algorithm $f_j$ trained using only $D_j$,
  - algorithm $f_n$ trained using all $D_j$'s,
  - a multi-task risk (e.g., average risk across tasks).


An algorithm $f$  .pu[multi-task] learns from data $D_n$   with respect to multi-task $T$ with overall performance measure $\mathcal{E}_T$, if $f$'s performance at task  $T$ improves with $D_n$."

--

$f_0$ .pu[multi-task] learns from data iff 

$$\frac{\mathcal{E}\_T( \lbrace f\_j \rbrace )}{\mathcal{E}\_T(f\_{n})} > 1.$$



---

### What is Lifelong Learning?


- Given
  - for $j=1,\ldots$ 
      - data $D_j$, possibly from different distributions, 
      - algorithm $f_j$ trained using only $D_j$,
  - algorithm  $f_n$ .pu[sequentially] trained using all $D_j$'s,
  - a multi-task risk (e.g., average risk across tasks).


An algorithm $f$  .pu[lifelong] learns from data $D_n$   with respect to lifelong task $T$ with overall performance measure $\mathcal{E}_T$, if $f$'s performance at task  $T$ improves with $D_n$."


--

$f$ .pu[lifelong] learns from data iff 

$$\frac{\mathcal{E}\_T( \lbrace f\_j \rbrace )}{\mathcal{E}\_T(f\_{n})} > 1.$$

---

###  Generalized Learning

<br> 

|       | $J>1$ | Sequential  | 
| :---: | :---: | :---:       | 
| Machine     | 0 | 0 | 
| Multi-task  | 1 | 0 | 
| Lifelong    | 1 | 1 | 








---

### Three Kinds of Lifelong Learning 

1. Supervised: setting is provided for each data sample 
2. Unsupervised: setting is not provided for any data sample 
3. Semi-supervised: setting is sometimes provided for some data samples 

--

We largely focus on supervised setting hereafter.

---
class: middle, inverse


## .center[General (Lifelong) Learning Machines]


---

### Machine Learning: Basic Set-Up

Given
1. A task $ T $ 
2. Observed data $ z\_1, z\_2, .., z\_n $

Assume
1. The $ z\_t $ are independent

---

### Constituents of  Learning Machines 

1.  .pu[representation]: data are transformed into a representation space
2.  .pu[transformer]: maps each point to the representation space
3.  .pu[decider]: takes an action using the transformed data

---

### Example 1 

Linear 2-class classification (we want to label black dot)

<img src="images/l2m_18mo/cml_all_data.png" style="width:500px" class="center"/>


---
#### Example 1.1 - Linear Discriminant Analysis

1. .pu[representation]: the real number line 
2. .pu[transformer]: projection matrix that maps each feature vector onto real number line 
3. .pu[decider]: threshold such that values above threshold are one class, and below are the other

<img src="images/l2m_18mo/cml_linear.jpg" style="width:600px" class="center"/>

<!-- diagram of linear classifier (projection from R^d to R then threshold) for two class, spherical gaussians -->
<!-- figure of problem -->
<!-- figure with representation -->
<!-- figure showing transformation of one data point to the line -->
<!-- figure showing decider acting on a the transformed data point -->

---

#### Example 1.2 - Decision Tree

1. .pu[representation]: a partition of $ \mathbb{R}^{d} $
2. .pu[transformer]: maps a data point to a cell of the partition
3. .pu[deciders]: plurality (or average) of elements per cell

<img src="images/l2m_18mo/cml-tree.jpg" style="width:650px" class="center"/>

<!-- diagram of a partition for two class, spherical gaussians -->
<!-- figure of representation (partition) -->
<!-- figure of a data point in a cell of the partition -->
<!-- figure of classifying a data point in a cell of the partition -->

---

#### Example 1.3 - Decision Forests 

1. .pu[representation]: a set of partitions of $ \mathbb{R}^{d} $ 
2. .pu[transformer]: maps a data point to a cell of each of the partitions
3. .pu[deciders]: average of decision trees (posterior probabilities)

<img src="images/l2m_18mo/cml_forest.jpg" style="width:600px" class="center"/>

Each tree uses a different subset of data to transform (partition) space



</textarea>
<!-- <script src="https://gnab.github.io/remark/downloads/remark-latest.min.js"></script> -->
<script src="remark-latest.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/contrib/auto-render.min.js"></script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css">
<script type="text/javascript">
  var options = {};
  var renderMath = function() {
    renderMathInElement(document.body);
    // or if you want to use $...$ for math,
    renderMathInElement(document.body, {delimiters: [ // mind the order of delimiters(!?)
        {left: "$$", right: "$$", display: true},
        {left: "$", right: "$", display: false},
        {left: "\\[", right: "\\]", display: true},
        {left: "\\(", right: "\\)", display: false},
    ]});
  }
  var slideshow = remark.create(options, renderMath);

</script>
</body>
</html>
